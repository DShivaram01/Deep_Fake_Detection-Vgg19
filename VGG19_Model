import cv2
import numpy as np
from keras.applications import VGG19
from keras.models import Model
from keras.layers import Dense, Flatten

# Load the pre-trained VGG19 model
base_model = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze the layers in the base model
for layer in base_model.layers:
    layer.trainable = False

# Add a new output layer on top of the pre-trained model
x = base_model.output
x = Flatten()(x)
predictions = Dense(1, activation='sigmoid')(x)
model = Model(inputs=base_model.input, outputs=predictions)

# Compile the model
model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

# Define the function to extract frames from the video
def extract_frames(video_path, num_frames=10):
    frames = []
    cap = cv2.VideoCapture(video_path)
    frame_count = 0
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        if frame_count % (cap.get(cv2.CAP_PROP_FRAME_COUNT) // num_frames) == 0:
            frame = cv2.resize(frame, (224, 224))
            frame = frame / 255.0
            frame = np.expand_dims(frame, axis=0)
            frames.append(frame)
        frame_count += 1
    cap.release()
    frames = np.concatenate(frames, axis=0)
    return frames

# Load the real videos dataset
real_videos_dir = ['/content/drive/MyDrive/FF++/original_sequences/youtube/c23/videos/000.mp4','/content/drive/MyDrive/FF++/original_sequences/youtube/c23/videos/001.mp4','/content/drive/MyDrive/FF++/original_sequences/youtube/c23/videos/002.mp4','/content/drive/MyDrive/FF++/original_sequences/youtube/c23/videos/003.mp4','/content/drive/MyDrive/FF++/original_sequences/youtube/c23/videos/004.mp4','/content/drive/MyDrive/FF++/original_sequences/youtube/c23/videos/005.mp4']
real_labels = np.zeros(6)  # Replace with the actual labels for real videos

real_frames = []
for video_path in real_videos_dir:
    frames = extract_frames(video_path)
    real_frames.append(frames)
real_frames = np.concatenate(real_frames, axis=0)


# Load the fake videos dataset
fake_videos_dir = ['/content/drive/MyDrive/FF++/manipulated_sequences/Deepfakes/c23/videos/000_003.mp4','/content/drive/MyDrive/FF++/manipulated_sequences/Deepfakes/c23/videos/001_870.mp4','/content/drive/MyDrive/FF++/manipulated_sequences/Deepfakes/c23/videos/002_006.mp4','/content/drive/MyDrive/FF++/manipulated_sequences/Deepfakes/c23/videos/003_000.mp4','/content/drive/MyDrive/FF++/manipulated_sequences/Deepfakes/c23/videos/004_982.mp4','/content/drive/MyDrive/FF++/manipulated_sequences/Deepfakes/c23/videos/005_010.mp4']
fake_labels = np.ones(6)  # Replace with the actual labels for fake videos

fake_frames = []
for video_path in fake_videos_dir:
    frames = extract_frames(video_path)
    fake_frames.append(frames)
fake_frames = np.concatenate(fake_frames, axis=0)

# Combine the real and fake frames and labels
all_frames = np.concatenate((real_frames, fake_frames), axis=0)
all_labels = np.concatenate((real_labels, fake_labels), axis=0)
all_labels = np.repeat(all_labels, all_frames.shape[0])
# Ensure that the number of samples matches
num_samples = len(all_frames)
all_frames = all_frames[:num_samples]
all_labels = all_labels[:num_samples]

# Verify the shapes of all_frames and all_labels
print('all_frames shape:', all_frames.shape)
print('all_labels shape:', all_labels.shape)

# Train the model
model.fit(
    all_frames,
    all_labels,
    batch_size=32,
    epochs=10,
    validation_split=0.2
)
from keras.applications import VGG16
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze the layers in the base model
for layer in base_model.layers:
    layer.trainable = False

# Add a new output layer on top of the pre-trained model
x = base_model.output
x = Flatten()(x)
predictions = Dense(1, activation='sigmoid')(x)
model = Model(inputs=base_model.input, outputs=predictions)


model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(
    all_frames,
    all_labels,
    batch_size=32,
    epochs=10,
    validation_split=0.2
)

# Evaluate the model on the validation data
score = model.evaluate(all_frames, all_labels)
print('Validation loss:', score[0])
print('Validation accuracy:', score[1])

